{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instead of manually implementing the gradient of the nlml as in 2D_example.ipynb, in this notebook we use symbolic differentiation instead.*\n",
    "\n",
    "*Results:*\n",
    "\n",
    "The algorithm actually works very elegantly with symbolic calculation.\n",
    "Nonetheless it is really slow (for n = 2 with only 500 epochs it takes around 1/2 hour with ADAM)\n",
    "To get good results we need at least n = 5 and the default epoch-value was 5000.\n",
    "So we can expect at least 10 hours for this. Since we want to deal with many points (n = 100 for sure), this method is not really viable.\n",
    "\n",
    "Also CG and Nelder didn't converge at all. \n",
    "\n",
    "At least with this notebook we could double-check the correctness of the gradient in 2D_example.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDE 1 - 2D with 4 parameters\n",
    "\n",
    "\n",
    "#### Problem Setup\n",
    "\n",
    "$\\phi u + u_{x} + u_{y,y} = f(x,y)$\n",
    "\n",
    "For the generation of our initial data samples we use:\n",
    "\n",
    "$\\phi = 2$ <br>\n",
    "$X_i := (x_i, y_i) \\in [0,1] \\times [0,1]$ for $i \\in \\{1, \\dotsc, n\\}$ <br>\n",
    "$u: \\mathbb{R}^2 \\rightarrow \\mathbb{R}, \\; u(x,y) = x^2 + y$ <br>\n",
    "$f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}, \\;f(x,y) = 2(x^2 + x + y)$\n",
    "\n",
    "and our known function values will be $\\{u(x_i,y_i), f(x_i,y_i)\\}_{i \\in \\{1, \\dotsc, n\\}}$\n",
    "\n",
    "We assume that $u$ can be represented as a Gaussian process with SE kernel.\n",
    "\n",
    "$u \\sim \\mathcal{GP}(0, k_{uu}(X_i, X_j; \\theta))$, where $\\theta = \\{\\sigma, l_x, l_y\\}$.\n",
    "\n",
    "And the linear operator:\n",
    "\n",
    "$\\mathcal{L}_X^{\\phi} = \\phi + \\partial_x + \\partial_{y,y}$\n",
    "\n",
    "so that\n",
    "\n",
    "$\\mathcal{L}_X^{\\phi} u = f$\n",
    "\n",
    "Problem at hand: Estimate $\\phi$ (the closer to $\\phi = 2$, the better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import pdb\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables: x, y, n, y_u, y_f, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parameters, that can be modified:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data samples:\n",
    "n = 2\n",
    "\n",
    "# Noise of our data:\n",
    "s = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data():\n",
    "    x = np.random.rand(n)\n",
    "    y = np.random.rand(n)\n",
    "    y_u = np.multiply(x,x) + y\n",
    "    y_f = 2*(np.multiply(x,x) + x + y)\n",
    "    return (x,y,y_u,y_f)\n",
    "(x,y,y_u,y_f) = simulate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Evaluate kernels\n",
    "\n",
    "I am squaring sigma, l_x and l_y right away due to the symbolic computation throughout:\n",
    "\n",
    "$k_{uu}(X_i, X_j; \\theta) = \\sigma^2 exp(-\\frac{1}{2l_x^2}(x_i-x_j)^2 - \\frac{1}{2l_y^2}(y_i-y_j)^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i, x_j, y_i, y_j, sigma, l_x, l_y, phi = sp.symbols('x_i x_j y_i y_j sigma l_x l_y phi')\n",
    "kuu_sym = sigma**2*sp.exp(-1/(2*l_x**2)*((x_i - x_j)**2) - 1/(2*l_y**2)*((y_i - y_j)**2))\n",
    "def kuu(x, y, sigma, l_x, l_y):\n",
    "    k = sp.zeros(n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            k[i,j] = kuu_sym.subs({x_i: x[i], x_j: x[j], y_i: y[i], y_j: y[j]})\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k_{ff}(X_i,X_j;\\theta,\\phi)$ <br>\n",
    "$= \\mathcal{L}_{X_i}^{\\phi} \\mathcal{L}_{X_j}^{\\phi} k_{uu}(X_i, X_j; \\theta)$ <br>\n",
    "$= \\phi^2k_{uu} + \\phi \\frac{\\partial}{\\partial x_i}k_{uu} + \\phi \\frac{\\partial^2}{\\partial y_i^2}k_{uu} + \\phi \\frac{\\partial}{\\partial x_j}k_{uu} + \\frac{\\partial^2}{\\partial x_i, x_j}k_{uu} + \\frac{\\partial^3}{\\partial y_i^2 \\partial x_j}k_{uu} + \\phi \\frac{\\partial^2}{\\partial y_j^2}k_{uu} + \\frac{\\partial^3}{\\partial x_i \\partial y_j^2}k_{uu} + \\frac{\\partial^4}{\\partial y_i^2 \\partial y_j^2}k_{uu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kff_sym = phi**2*kuu_sym \\\n",
    "        + phi*sp.diff(kuu_sym, x_i) \\\n",
    "        + phi*sp.diff(kuu_sym, y_i, y_i) \\\n",
    "        + phi*sp.diff(kuu_sym, x_j) \\\n",
    "        + sp.diff(kuu_sym, x_i, x_j) \\\n",
    "        + sp.diff(kuu_sym, y_i, y_i, x_j) \\\n",
    "        + phi*sp.diff(kuu_sym, y_j, y_j) \\\n",
    "        + sp.diff(kuu_sym, x_i, y_j, y_j) \\\n",
    "        + sp.diff(kuu_sym, y_i, y_i, y_j, y_j)\n",
    "def kff(x, y, sigma, l_x, l_y, phi):\n",
    "    k = sp.zeros(n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            k[i,j] = kff_sym.subs({x_i: x[i], x_j: x[j], y_i: y[i], y_j: y[j]})\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k_{fu}(X_i,X_j;\\theta,\\phi) \\\\\n",
    "= \\mathcal{L}_{X_i}^{\\phi} k_{uu}(X_i, X_j; \\sigma) \\\\\n",
    "= \\phi k_{uu} + \\frac{\\partial}{\\partial x_i}k_{uu} + \\frac{\\partial^2}{\\partial y_i^2}k_{uu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfu_sym = phi*kuu_sym \\\n",
    "        + sp.diff(kuu_sym, x_i) \\\n",
    "        + sp.diff(kuu_sym, y_i, y_i)\n",
    "def kfu(x, y, sigma, l_x, l_y, phi):\n",
    "    k = sp.zeros(n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            k[i,j] = kfu_sym.subs({x_i: x[i], x_j: x[j], y_i: y[i], y_j: y[j]})\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kuf(x, y, sigma, l_x, l_y, phi):\n",
    "    return kfu(x, y, sigma, l_x, l_y, phi).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compute NLML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the covariance matrix K and the (symbolic) nlml function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(sigma, l_x, l_y, phi, s):\n",
    "    K_mat = sp.Matrix(sp.BlockMatrix([\n",
    "        [kuu(x, y, sigma, l_x, l_y) + s*np.eye(n), kuf(x, y, sigma, l_x, l_y, phi)],\n",
    "        [kfu(x, y, sigma, l_x, l_y, phi), kff(x, y, sigma, l_x, l_y, phi) + s*np.eye(n)]\n",
    "    ]))\n",
    "    return K_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlml(sigma, l_x, l_y, phi, s):\n",
    "    #sigma, l_x, l_y, phi = params\n",
    "    y_con = sp.Matrix(np.concatenate((y_u, y_f)))\n",
    "    nlml1 = sp.log(sp.det(K(sigma, l_x, l_y, phi, s)))\n",
    "    nlml2 = y_con.T*(K(sigma, l_x, l_y, phi, s).inv())*y_con\n",
    "    nlml = nlml1 + nlml2[0,0]\n",
    "    return nlml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nonlinear Conjugate Gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlml_sigma, nlml_l_x, nlml_l_y, nlml_phi = [sp.diff(nlml(sigma, l_x, l_y, phi, s), l) for \n",
    "                                            l in [sigma, l_x, l_y, phi]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_nlml(par):\n",
    "    nlml_wp_sigma = lambda v: nlml_sigma.subs({sigma:v[0], l_x:v[1], l_y:v[2], phi:v[3]})\n",
    "    nlml_wp_l_x = lambda v: nlml_l_x.subs({sigma:v[0], l_x:v[1], l_y:v[2], phi:v[3]})\n",
    "    nlml_wp_l_y = lambda v: nlml_l_y.subs({sigma:v[0], l_x:v[1], l_y:v[2], phi:v[3]})\n",
    "    nlml_wp_phi = lambda v: nlml_phi.subs({sigma:v[0], l_x:v[1], l_y:v[2], phi:v[3]})\n",
    "    \n",
    "    # Minimize (to be exact np.linalg.norm) can't deal with the scipy float type \n",
    "    # (=>'Float' object has no attribute 'sqrt')\n",
    "    out = np.array([nlml_wp_sigma(par), nlml_wp_l_x(par), nlml_wp_l_y(par), nlml_wp_phi(par)], \n",
    "                   dtype=np.float64)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-24.54939927,  -3.50291112,   7.92713189,  -7.17654828])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_nlml((0.70560699, -1.1791943 ,  1.74529172,  2.53646637))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the conjugate gradient method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_restarts_CG(n=10): \n",
    "    nlml_wp = lambda v: nlml(sigma, l_x, l_y, phi, s).subs({sigma:v[0], l_x:v[1], l_y:v[2], phi:v[3]})\n",
    "    all_results = []\n",
    "    for it in range(0,n):\n",
    "        all_results.append(minimize(nlml_wp, np.random.rand(4), jac = grad_nlml,  method=\"CG\"))\n",
    "    filtered_results = [m for m in all_results if 0==m.status]\n",
    "    return min(filtered_results, key = lambda x: x.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "m = minimize_restarts_CG(2)\n",
    "t_CG = time.time() - t1\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adams(grad, init, n_epochs=500, eta=10**-4, gamma=0.9,beta=0.99,epsilon=10**-8,noise_strength=0):\n",
    "    params=np.array(init)\n",
    "    param_traj=np.zeros([n_epochs+1,4])\n",
    "    param_traj[0,]=init\n",
    "    v=0;\n",
    "    grad_sq=0;\n",
    "    for j in range(n_epochs):\n",
    "        noise=noise_strength*np.random.randn(params.size)\n",
    "        g=np.array(grad(params))+noise\n",
    "        v=gamma*v+(1-gamma)*g\n",
    "        grad_sq=beta*grad_sq+(1-beta)*g*g\n",
    "        v_hat=v/(1-gamma)\n",
    "        grad_sq_hat=grad_sq/(1-beta)\n",
    "        params=params-eta*np.divide(v_hat,np.sqrt(grad_sq_hat+epsilon))\n",
    "        param_traj[j+1,]=params\n",
    "    return param_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running ADAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_restarts_adam(n=10):\n",
    "    all_results = []\n",
    "    for it in range(0,n):\n",
    "        all_results.append(adams(grad_nlml, np.random.rand(4)))\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "m = minimize_restarts_adam(2)\n",
    "t_adam = time.time() - t1\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_CG,'\\n',t_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
