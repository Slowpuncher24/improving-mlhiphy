{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDE 1 - 2D\n",
    "\n",
    "\n",
    "#### Problem Setup\n",
    "\n",
    "$\\phi u + u_{x} + u_{y,y} = f(x,y)$\n",
    "\n",
    "For the generation of our initial data samples we use:\n",
    "\n",
    "$\\phi = 2$ <br>\n",
    "$u: \\mathbb{R}^2 \\rightarrow \\mathbb{R}, \\; u(x,y) = x^2 + y$ <br>\n",
    "$f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}, \\;f(x,y) = 2(x^2 + x + y)$ <br>\n",
    "$X_i := (x_i, y_i) \\in [0,1] \\times [0,1] \\subseteq \\mathbb{R}^2$ for $i \\in \\{1, \\dotsc, n\\}$ \n",
    "\n",
    "and our known function values will be $\\{u(x_i,y_i), f(x_i,y_i)\\}_{i \\in \\{1, \\dotsc, n\\}}$.\n",
    "\n",
    "We assume that $u$ can be represented as a Gaussian process with Mat√©rn kernel, where $\\nu = 5/2$.\n",
    "\n",
    "$u \\sim \\mathcal{GP}(0, k_{uu}(X_i, X_j; \\theta))$, where $\\theta = \\{\\sigma, l_x, l_y\\}$.\n",
    "\n",
    "Set the linear operator to:\n",
    "\n",
    "$\\mathcal{L}_X^{\\phi} := \\phi + \\partial_x + \\partial_{y,y}$\n",
    "\n",
    "so that\n",
    "\n",
    "$\\mathcal{L}_X^{\\phi} u = f$\n",
    "\n",
    "Problem at hand: Estimate $\\phi$ (we expect $\\phi = 2$).\n",
    "\n",
    "\n",
    "#### Step 1: Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from scipy.linalg import solve_triangular\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables: x, y, n, y_u, y_f, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parameters, that can be modified:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data samples:\n",
    "n = 10\n",
    "\n",
    "# Noise of our data:\n",
    "s = 0\n",
    "\n",
    "# Circumventing evaluations of kernel-derivatives at zero:\n",
    "corr_nan = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data():\n",
    "    x = np.random.rand(n)\n",
    "    y = np.random.rand(n)\n",
    "    y_u = np.multiply(x,x) + y\n",
    "    y_f = 2*(np.multiply(x,x) + x + y)\n",
    "    return (x,y,y_u,y_f)\n",
    "(x,y,y_u,y_f) = simulate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Evaluate kernels\n",
    "\n",
    "$k_{uu}(X_i, X_j; \\theta) = \\sigma \\left( 1+ \\sqrt{5}r_l + \\frac{5}{3}r_l^2 \\right) \\exp \\left( -\\sqrt{5}r_l \\right)$, where:\n",
    "\n",
    "$r_l = \\sqrt{\\frac{1}{l_x^2}(x_i-x_j)^2 + \\frac{1}{l_y^2}(y_i-y_j)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i, x_j, y_i, y_j, sigma, l_x, l_y, phi = sp.symbols('x_i x_j y_i y_j sigma l_x l_y phi')\n",
    "# kuu_sym = sigma*sp.exp(-1/(2*l_x)*((x_i - x_j)**2) - 1/(2*l_y)*((y_i - y_j)**2))\n",
    "r_l = sp.sqrt((x_i - x_j)**2/l_x + (y_i - y_j)**2/l_y)\n",
    "kuu_sym = sigma*(1 + sp.sqrt(5)*r_l + 5/3*r_l**2)*sp.exp(-sp.sqrt(5)*r_l)\n",
    "kuu_fn = sp.lambdify((x_i, x_j, y_i, y_j, sigma, l_x, l_y), kuu_sym, \"numpy\")\n",
    "def kuu(x, y, sigma, l_x, l_y):\n",
    "    k = np.zeros((x.size, x.size))\n",
    "    for i in range(x.size):\n",
    "        for j in range(x.size):\n",
    "            k[i,j] = kuu_fn(x[i], x[j], y[i], y[j], sigma, l_x, l_y)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k_{ff}(X_i,X_j;\\theta,\\phi) \\\\\n",
    "= \\mathcal{L}_{X_i}^{\\phi} \\mathcal{L}_{X_j}^{\\phi} k_{uu}(X_i, X_j; \\theta) \\\\\n",
    "= \\phi^2k_{uu} + \\phi \\frac{\\partial}{\\partial x_i}k_{uu} + \\phi \\frac{\\partial^2}{\\partial y_i^2}k_{uu} + \\phi \\frac{\\partial}{\\partial x_j}k_{uu} + \\frac{\\partial^2}{\\partial x_i, x_j}k_{uu} + \\frac{\\partial^3}{\\partial y_i^2 \\partial x_j}k_{uu} + \\phi \\frac{\\partial^2}{\\partial y_j^2}k_{uu} + \\frac{\\partial^3}{\\partial x_i \\partial y_j^2}k_{uu} + \\frac{\\partial^4}{\\partial y_i^2 \\partial y_j^2}k_{uu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kff_sym = phi**2*kuu_sym \\\n",
    "        + phi*sp.diff(kuu_sym, x_i) \\\n",
    "        + phi*sp.diff(kuu_sym, y_i, y_i) \\\n",
    "        + phi*sp.diff(kuu_sym, x_j) \\\n",
    "        + sp.diff(kuu_sym, x_i, x_j) \\\n",
    "        + sp.diff(kuu_sym, y_i, y_i, x_j) \\\n",
    "        + phi*sp.diff(kuu_sym, y_j, y_j) \\\n",
    "        + sp.diff(kuu_sym, x_i, y_j, y_j) \\\n",
    "        + sp.diff(kuu_sym, y_i, y_i, y_j, y_j)\n",
    "kff_fn = sp.lambdify((x_i, x_j, y_i, y_j, sigma, l_x, l_y, phi), kff_sym, \"numpy\")\n",
    "def kff(x, y, sigma, l_x, l_y, phi):\n",
    "    k = np.zeros((x.size, x.size))\n",
    "    for i in range(x.size):\n",
    "        for j in range(x.size):\n",
    "            if i == j:\n",
    "                k[i,j] = kff_fn(x[i], x[j] + corr_nan, y[i], y[j] + corr_nan, sigma, l_x, l_y, phi)\n",
    "            else:\n",
    "                k[i,j] = kff_fn(x[i], x[j], y[i], y[j], sigma, l_x, l_y, phi)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k_{fu}(X_i,X_j;\\theta,\\phi) \\\\\n",
    "= \\mathcal{L}_{X_i}^{\\phi} k_{uu}(X_i, X_j; \\theta) \\\\\n",
    "= \\phi k_{uu} + \\frac{\\partial}{\\partial x_i}k_{uu} + \\frac{\\partial^2}{\\partial y_i^2}k_{uu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfu_sym = phi*kuu_sym \\\n",
    "        + sp.diff(kuu_sym, x_i) \\\n",
    "        + sp.diff(kuu_sym, y_i, y_i)\n",
    "kfu_fn = sp.lambdify((x_i, x_j, y_i, y_j, sigma, l_x, l_y, phi), kfu_sym, \"numpy\")\n",
    "def kfu(x, y, sigma, l_x, l_y, phi):\n",
    "    k = np.zeros((x.size, x.size))\n",
    "    for i in range(x.size):\n",
    "        for j in range(x.size):\n",
    "            if i == j:\n",
    "                k[i,j] = kfu_fn(x[i], x[j] + corr_nan, y[i], y[j] + corr_nan, sigma, l_x, l_y, phi)\n",
    "            else:\n",
    "                k[i,j] = kfu_fn(x[i], x[j], y[i], y[j], sigma, l_x, l_y, phi)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kuf(x, y, sigma, l_x, l_y, phi):\n",
    "    return kfu(x, y, sigma, l_x, l_y, phi).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Computing the negative log-likelihood \n",
    "(with block matrix inversion, Cholesky decomposition, potentially SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the block-inversion technique: Let\n",
    "$ K = \\begin{pmatrix} K_{uu} & K_{uf} \\\\ K_{fu} & K_{ff} \\end{pmatrix} = \\begin{pmatrix} A & B \\\\ B^T & C \\end{pmatrix}$. \n",
    "\n",
    "Then $det(K) = det(A) det(C-B^T A^{-1} B)$.\n",
    "\n",
    "$K^{-1} = \\begin{pmatrix} A^{-1} + A^{-1} B(C-B^T A^{-1} B)^{-1}B^T A^{-1} & -A^{-1}B(C-B^T A^{-1} B)^{-1} \\\\\n",
    "            -(C - B^T A^{-1}B)^{-1}B^T A^{-1} & (C-B^T A^{-1} B)^{-1} \\end{pmatrix}$\n",
    "            \n",
    "So it suffices to invert $A$ and $C-B^T A^{-1} B$.\n",
    "\n",
    "A theorem about Schur-complements ensures that $K$ positive-definite implies the positive-definiteness of $K/A = C-B^T A^{-1} B$ as well, so Cholesky should work as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlml(params):\n",
    "    \n",
    "    sigma_exp = np.exp(params[0])\n",
    "    l_x_exp = np.exp(params[1])\n",
    "    l_y_exp = np.exp(params[2])\n",
    "    # phi = params[3]\n",
    "    \n",
    "    A = kuu(x, y, sigma_exp, l_x_exp, l_y_exp) + s*np.eye(n)\n",
    "    B = kfu(x, y, sigma_exp, l_x_exp, l_y_exp, params[3]).T\n",
    "    C = kff(x, y, sigma_exp, l_x_exp, l_y_exp, params[3]) + s*np.eye(n)\n",
    "    \n",
    "    # Inversion of A\n",
    "    A_inv = np.zeros((n, n))\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(A)\n",
    "        L_inv = solve_triangular(L, np.identity(n), lower=True) # Slight performance boost \n",
    "                                                                # over np.linalg.inv\n",
    "        A_inv = L_inv.T @ L_inv\n",
    "        logdet_A = 2*np.log(np.abs(np.diag(L))).sum()\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Inverse of K via SVD\n",
    "        u, s_mat, vt = np.linalg.svd(A)\n",
    "        A_inv = vt.T @ np.linalg.inv(np.diag(s_mat)) @ u.T\n",
    "        logdet_A = np.log(s_mat).sum()\n",
    "        \n",
    "    # Inversion of $C-B^T A^{-1} B$\n",
    "    KA_inv = np.zeros((n, n))\n",
    "    KA = C - B.T @ A_inv @ B\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(KA)\n",
    "        L_inv = solve_triangular(L, np.identity(n), lower=True) # Slight performance boost \n",
    "                                                                # over np.linalg.inv\n",
    "        KA_inv = L_inv.T @ L_inv\n",
    "        logdet_KA = 2*np.log(np.abs(np.diag(L))).sum()\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Inverse of K via SVD\n",
    "        u, s_mat, vt = np.linalg.svd(KA)\n",
    "        KA_inv = vt.T @ np.linalg.inv(np.diag(s_mat)) @ u.T\n",
    "        logdet_KA = np.log(s_mat).sum()\n",
    "        \n",
    "    # Piecing it together\n",
    "    T = A_inv @ B @ KA_inv\n",
    "    yKy = y_u @ (A_inv + T @ B.T @ A_inv) @ y_u - 2*y_u @ T @ y_f + y_f @ KA_inv @ y_f\n",
    "    \n",
    "    return (yKy + logdet_A + logdet_KA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Nelder-Mead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nfeval = 1\n",
    "def callbackF(Xi):\n",
    "    global Nfeval\n",
    "    print('{0:4d}   {1: 3.6f}   {2: 3.6f}   {3: 3.6f}   {4: 3.6f}'.\n",
    "          format(Nfeval, Xi[0], Xi[1], Xi[2], Xi[3]))\n",
    "    Nfeval += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "m_n = opt.minimize(nlml, np.random.rand(4), method=\"Nelder-Mead\", callback = callbackF,\n",
    "                                        options={'maxfev':5000, 'fatol':0.001, 'xatol':0.001})\n",
    "t_Nelder = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred parameter: 1.9931\n"
     ]
    }
   ],
   "source": [
    "print('Inferred parameter: %.4f' % m_n.x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 362 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Time: %d seconds' % t_Nelder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
